{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/msKNSs8rmJ5m/giphy.gif\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'images/thinking.jpeg'></img>\n",
    "\n",
    "#### Scenario: You wok for a international political consultant.  Your work is on the level and not shady at all.  Scott, a friendly coworker, is trying to sort through news articles to quickly filter political from non-political articles. He has heard you possess some solid NLP chops, and has asked for your help in automating the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "<p float = 'left'> What type of problem is this?</p>\n",
    "<p float = 'left'> What steps do you anticipate carrying out? </p>\n",
    "<p float = 'left'> What challenges do you foresee? </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "<img src = 'https://media.giphy.com/media/WqLmcthJ7AgQKwYJbb/giphy.gif' alt=\"Drawing\" style=\"width: 300px;\"  float = 'right'> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vanilla Python Text Exploration\n",
    "\n",
    "Explore the texts in the example texts by:\n",
    "\n",
    "1. Creating a list of words in each text.\n",
    "2. Counting the number of occurences of the words.  \n",
    "3. Ordering the words by number of occurences.\n",
    "4. Comparing the counts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "with open('text_examples/A.txt', 'r') as read_file:\n",
    "    text = read_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now you order the number of occurences and compare the texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Bag of Words</h2>\n",
    "\n",
    "<img src = \"images/bag_of_word.jpg\"></img>\n",
    "\n",
    "What is the problem with text in relation to machine learning?\n",
    "BOW takes a text, breaks it up into small pieces (words, bigrams, stems, lemma), an converts it into counts.  These counts can then be fed into our familiar machine learning algorithms.\n",
    "\n",
    "Question: Did any algorithm pop into your head that might be particularly suited to bags of words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steps for creating a bag of words\n",
    "\n",
    "1. make lowercase \n",
    "2. remove punctuation\n",
    "3. remove stopwords\n",
    "4. apply stemmer/lemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To help us with the above steps, we will introduce a new library, **NLTK**  [documentation](https://www.nltk.org/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#We will come back to the articles later, but to practice preparing a text, \n",
    "#let's use another of the NLTK resources (https://www.nltk.org/book/ch02.html)\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, regexp_tokenize\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "# text = nltk.corpus.gutenberg.raw(<fill_in>).replace('\\n',' ')[:2000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## tokenize the text\n",
    "text_tokens = word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# or use regexp which can take care of punctuation removal as well.\n",
    "#https://regexr.com/\n",
    "pattern = (\"([a-zA-Z]+(?:'[a-z]+)?)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Make Lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Remove punctuation\n",
    "# 3. Remove Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# stopwords.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stemmers/Lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Stemmers/Lemmatizers\n",
    "## why would I use a stemmer and not a lemmatizer at all times?\n",
    "# raw_text = nltk.corpus.gutenberg.raw()\n",
    "from nltk.stem import *\n",
    "\n",
    "\n",
    "example = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "          'plotted']\n",
    "\n",
    "p_stem = PorterStemmer()\n",
    "stemmed_words = [p_stem.stem(word) for word in example]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Porter Stemmer: Least aggressive stemmer.\n",
    "Snowball stemmer: more aggressive in how it stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Lemmatizers\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "example = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "          'plotted']\n",
    "\n",
    "from nltk import pos_tag\n",
    "# Problem with POS\n",
    "wordnet_lemmatizer.lemmatize(example[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Frequency Distributions the easy way\n",
    "\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing using data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = ['the bread week winner goes to the final', \n",
    "        'the winner never has a soggy bottom', \n",
    "        'the contestants at the bottom were poor bread bakers' ]\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(docs)\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each word is a dimension!\n",
    "\n",
    "![word](https://media.giphy.com/media/xT1R9ERHwyzbCkIwla/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
